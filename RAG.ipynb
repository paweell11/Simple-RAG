{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bccf73b-e6ee-4579-8ea9-a1b5c4732d57",
   "metadata": {},
   "source": [
    "# **LOAD DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f727201-0488-48f4-ba93-6085976b80f5",
   "metadata": {},
   "source": [
    "**First, we need to load the dataset. I chose a small dataset from Wikipedia - specifically the \"rahular/simple-wikipedia\" dataset from HuggingFace Hub.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a034356b-5b3c-4f58-8f50-26339b0fa478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rahular/simple-wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc85e0-f3f9-4fe3-bd29-05d819cac5d6",
   "metadata": {},
   "source": [
    "### **Key Information About Our Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39b2f2b-8578-4de8-a078-c7005b2ef423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Available splits: ['train']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e3fae8-20a1-4cd3-ba14-b8459f1cd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 769764\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length: {len(dataset[\"train\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2039b53a-0f4b-44af-ae10-f800c9abdaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset features: {'text': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46000122-29da-4301-b7ce-22b83b66e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record (index 558): {'text': 'Plants are also multicellular eukaryotic organisms, but live by using light, water and basic elements to make their tissues.'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample record (index 558): {dataset['train'][558]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169c9d9b-9289-492a-8a51-c65df3f7efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty records: 0\n"
     ]
    }
   ],
   "source": [
    "empty = [i for i in range(len(dataset[\"train\"])) if not dataset[\"train\"][i][\"text\"].strip()]\n",
    "print(f\"Empty records: {len(empty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5799db-2d66-4e84-a116-cee9441245a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data as DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April is the fourth month of the year, and com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April always begins on the same day of week as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April's flowers are the Sweet Pea and Daisy. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April comes between March and May, making it t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                              April\n",
       "1  April is the fourth month of the year, and com...\n",
       "2  April always begins on the same day of week as...\n",
       "3  April's flowers are the Sweet Pea and Daisy. I...\n",
       "4  April comes between March and May, making it t..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset[\"train\"][:5])\n",
    "print(\"Sample data as DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebae6b4-a846-404a-bac7-a70d1e0fe764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245339ae-c122-4b25-85cf-bfd19322071f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cbc64-16b9-4fe4-879a-2c1699216353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4b1cb-d15e-4019-9038-5b8ba2fefe49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f78c5a0-4764-42bf-82ed-3862c50c8a79",
   "metadata": {},
   "source": [
    "# **Document Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6718f04-76e9-4a82-a2c4-9f560290f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 769764 LangChain documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document as LangChainDocument\n",
    "    \n",
    "\n",
    "langchain_documents = []\n",
    "for i, record in enumerate(dataset[\"train\"]):\n",
    "    text_content = record[\"text\"]\n",
    "        \n",
    "    if not text_content.strip():\n",
    "        continue\n",
    "        \n",
    "    metadata = {\n",
    "        \"source\": \"simple_wikipedia\",\n",
    "        \"original_index\": i,\n",
    "        \"text_length\": len(text_content),\n",
    "    }\n",
    "        \n",
    "    doc = LangChainDocument(\n",
    "        page_content=text_content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "        \n",
    "    langchain_documents.append(doc)\n",
    "    \n",
    "print(f\"✅ Created {len(langchain_documents)} LangChain documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f4ede-d4ad-4594-9c20-aefbb45ec326",
   "metadata": {},
   "source": [
    "# **Text Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1413594-99d7-41d6-a391-08a469793b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-empty documents: 769764\n",
      "\n",
      "DOCUMENT LENGTH STATISTICS (in characters):\n",
      "--------------------------------------------------\n",
      "Minimum length: 1 characters\n",
      "Maximum length: 10,570 characters\n",
      "Average length: 183 characters\n",
      "Median length: 127 characters\n",
      "Standard deviation: 198 characters\n",
      "\n",
      "Percentiles:\n",
      "  10th percentile: 12 characters\n",
      "  25th percentile: 24 characters\n",
      "  50th percentile: 127 characters\n",
      "  75th percentile: 271 characters\n",
      "  90th percentile: 432 characters\n",
      "  95th percentile: 558 characters\n",
      "  99th percentile: 872 characters\n"
     ]
    }
   ],
   "source": [
    "# Get all text lengths from the dataset\n",
    "text_lengths = []\n",
    "for i, record in enumerate(dataset[\"train\"]):\n",
    "    text = record[\"text\"]\n",
    "    if text.strip():  # Skip empty records\n",
    "        text_lengths.append(len(text))\n",
    "\n",
    "print(f\"Total non-empty documents: {len(text_lengths)}\")\n",
    "\n",
    "# =====================================================================\n",
    "# LENGTH STATISTICS\n",
    "# =====================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nDOCUMENT LENGTH STATISTICS (in characters):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Minimum length: {min(text_lengths):,} characters\")\n",
    "print(f\"Maximum length: {max(text_lengths):,} characters\")\n",
    "print(f\"Average length: {np.mean(text_lengths):,.0f} characters\")\n",
    "print(f\"Median length: {np.median(text_lengths):,.0f} characters\")\n",
    "print(f\"Standard deviation: {np.std(text_lengths):,.0f} characters\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(text_lengths, p)\n",
    "    print(f\"  {p}th percentile: {value:,.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f1a16-e948-4b90-8297-b4d1a30e290f",
   "metadata": {},
   "source": [
    "### Why Text Splitting is Not Necessary for Our Dataset\n",
    "\n",
    "After analyzing our Simple Wikipedia dataset, we found that text splitting (chunking) is not necessary for this particular dataset. Our dataset contains 769,764 non-empty documents with an average length of only 183 characters (46 tokens) and a median of 127 characters (32 tokens). Even the 95th percentile is just 558 characters (140 tokens), which is well below the typical embedding model limits of 512-8192 tokens.\n",
    "\n",
    "\n",
    "Since most embedding models can easily handle documents of this size, splitting our already short Wikipedia articles would actually be counterproductive. Text splitting is typically needed when documents are very long (>1000 characters) or exceed model token limits, but our dataset doesn't meet these criteria. Keeping the documents intact preserves the complete context of each Wikipedia article, results in better embeddings, and simplifies our RAG pipeline. We can proceed directly to embedding generation without any chunking steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b3fb2-1dcb-443f-849f-2cb099a8b7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d624cc-a023-402a-b953-c9f942f862e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004d71f7-c09d-4a2d-be58-06c985bbb5d8",
   "metadata": {},
   "source": [
    "# **Embedding Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0763f888-a240-44be-9c43-2f7633d91897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\",device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae0a320-0cda-4ac0-a3ae-b19ce9cec465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Model device: {embedding_model.device}\")\n",
    "# print(f\"Model is on GPU: {next(embedding_model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5069d-d3cf-4023-9665-e8559bca72ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b239c57-a426-46d5-af3a-5f7b7ea2b0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2528a31-55eb-42f8-b7f7-1894234e8058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca9c02e-e04b-4ce1-b03b-1d6eea83c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [doc.page_content for doc in langchain_documents]\n",
    "# embeddings = embedding_model.encode(texts)\n",
    "# print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88e9fb-cf26-415a-8b6f-fce0a4940455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdb3d9d6-5711-4ec9-b63f-8de6524c75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae32f8c-9bd3-4c5f-b0f8-f1455e2ae9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model kwargs: {'device': 'cuda'}\n",
      "Model name: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model kwargs: {embedding_function.model_kwargs}\")\n",
    "print(f\"Model name: {embedding_function.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5950d04a-e320-4979-9bb4-f1cea74b3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in langchain_documents]\n",
    "embeddings = embedding_function.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11144439-22df-4e94-aa81-0fe0c7fe7c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 769764\n",
      "Embedding dimension: 384\n",
      "Shape equivalent: (769764, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"Shape equivalent: ({len(embeddings)}, {len(embeddings[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384eea30-2d20-4d57-b220-0ed6c6407b6b",
   "metadata": {},
   "source": [
    "# PORÓB JAKIES POROWNAIA PODOBNYCH ZDAŃ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ee883-15ad-41c4-bb22-8db5e2dffda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99ce21bd-7ca4-465d-95f7-c62721e525c2",
   "metadata": {},
   "source": [
    "# **Vector Database Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e9b4b33-2de5-4872-94ef-1058285ef0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Najpierw musisz stworzyć wrapper dla SentenceTransformer\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# # Stwórz embedding function kompatybilną z LangChain\n",
    "# embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa2ce52-97e1-435f-a1da-59b9a6e2b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "persist_dir = \"./chroma_wikipedia_db\"\n",
    "\n",
    "# Usuń starą bazę, jeśli istnieje\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"simple_wikipedia_collection\",\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=persist_dir, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4dc5cf-7e91-4ab6-a14f-5247fa23b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dodano batch 1: dokumenty 0 - 5000\n",
      "Dodano batch 2: dokumenty 5000 - 10000\n",
      "Dodano batch 3: dokumenty 10000 - 15000\n",
      "Dodano batch 4: dokumenty 15000 - 20000\n",
      "Dodano batch 5: dokumenty 20000 - 25000\n",
      "Dodano batch 6: dokumenty 25000 - 30000\n",
      "Dodano batch 7: dokumenty 30000 - 35000\n",
      "Dodano batch 8: dokumenty 35000 - 40000\n",
      "Dodano batch 9: dokumenty 40000 - 45000\n",
      "Dodano batch 10: dokumenty 45000 - 50000\n",
      "Dodano batch 11: dokumenty 50000 - 55000\n",
      "Dodano batch 12: dokumenty 55000 - 60000\n",
      "Dodano batch 13: dokumenty 60000 - 65000\n",
      "Dodano batch 14: dokumenty 65000 - 70000\n",
      "Dodano batch 15: dokumenty 70000 - 75000\n",
      "Dodano batch 16: dokumenty 75000 - 80000\n",
      "Dodano batch 17: dokumenty 80000 - 85000\n",
      "Dodano batch 18: dokumenty 85000 - 90000\n",
      "Dodano batch 19: dokumenty 90000 - 95000\n",
      "Dodano batch 20: dokumenty 95000 - 100000\n",
      "Dodano batch 21: dokumenty 100000 - 105000\n",
      "Dodano batch 22: dokumenty 105000 - 110000\n",
      "Dodano batch 23: dokumenty 110000 - 115000\n",
      "Dodano batch 24: dokumenty 115000 - 120000\n",
      "Dodano batch 25: dokumenty 120000 - 125000\n",
      "Dodano batch 26: dokumenty 125000 - 130000\n",
      "Dodano batch 27: dokumenty 130000 - 135000\n",
      "Dodano batch 28: dokumenty 135000 - 140000\n",
      "Dodano batch 29: dokumenty 140000 - 145000\n",
      "Dodano batch 30: dokumenty 145000 - 150000\n",
      "Dodano batch 31: dokumenty 150000 - 155000\n",
      "Dodano batch 32: dokumenty 155000 - 160000\n",
      "Dodano batch 33: dokumenty 160000 - 165000\n",
      "Dodano batch 34: dokumenty 165000 - 170000\n",
      "Dodano batch 35: dokumenty 170000 - 175000\n",
      "Dodano batch 36: dokumenty 175000 - 180000\n",
      "Dodano batch 37: dokumenty 180000 - 185000\n",
      "Dodano batch 38: dokumenty 185000 - 190000\n",
      "Dodano batch 39: dokumenty 190000 - 195000\n",
      "Dodano batch 40: dokumenty 195000 - 200000\n",
      "Dodano batch 41: dokumenty 200000 - 205000\n",
      "Dodano batch 42: dokumenty 205000 - 210000\n",
      "Dodano batch 43: dokumenty 210000 - 215000\n",
      "Dodano batch 44: dokumenty 215000 - 220000\n",
      "Dodano batch 45: dokumenty 220000 - 225000\n",
      "Dodano batch 46: dokumenty 225000 - 230000\n",
      "Dodano batch 47: dokumenty 230000 - 235000\n",
      "Dodano batch 48: dokumenty 235000 - 240000\n",
      "Dodano batch 49: dokumenty 240000 - 245000\n",
      "Dodano batch 50: dokumenty 245000 - 250000\n",
      "Dodano batch 51: dokumenty 250000 - 255000\n",
      "Dodano batch 52: dokumenty 255000 - 260000\n",
      "Dodano batch 53: dokumenty 260000 - 265000\n",
      "Dodano batch 54: dokumenty 265000 - 270000\n",
      "Dodano batch 55: dokumenty 270000 - 275000\n",
      "Dodano batch 56: dokumenty 275000 - 280000\n",
      "Dodano batch 57: dokumenty 280000 - 285000\n",
      "Dodano batch 58: dokumenty 285000 - 290000\n",
      "Dodano batch 59: dokumenty 290000 - 295000\n",
      "Dodano batch 60: dokumenty 295000 - 300000\n",
      "Dodano batch 61: dokumenty 300000 - 305000\n",
      "Dodano batch 62: dokumenty 305000 - 310000\n",
      "Dodano batch 63: dokumenty 310000 - 315000\n",
      "Dodano batch 64: dokumenty 315000 - 320000\n",
      "Dodano batch 65: dokumenty 320000 - 325000\n",
      "Dodano batch 66: dokumenty 325000 - 330000\n",
      "Dodano batch 67: dokumenty 330000 - 335000\n",
      "Dodano batch 68: dokumenty 335000 - 340000\n",
      "Dodano batch 69: dokumenty 340000 - 345000\n",
      "Dodano batch 70: dokumenty 345000 - 350000\n",
      "Dodano batch 71: dokumenty 350000 - 355000\n",
      "Dodano batch 72: dokumenty 355000 - 360000\n",
      "Dodano batch 73: dokumenty 360000 - 365000\n",
      "Dodano batch 74: dokumenty 365000 - 370000\n",
      "Dodano batch 75: dokumenty 370000 - 375000\n",
      "Dodano batch 76: dokumenty 375000 - 380000\n",
      "Dodano batch 77: dokumenty 380000 - 385000\n",
      "Dodano batch 78: dokumenty 385000 - 390000\n",
      "Dodano batch 79: dokumenty 390000 - 395000\n",
      "Dodano batch 80: dokumenty 395000 - 400000\n",
      "Dodano batch 81: dokumenty 400000 - 405000\n",
      "Dodano batch 82: dokumenty 405000 - 410000\n",
      "Dodano batch 83: dokumenty 410000 - 415000\n",
      "Dodano batch 84: dokumenty 415000 - 420000\n",
      "Dodano batch 85: dokumenty 420000 - 425000\n",
      "Dodano batch 86: dokumenty 425000 - 430000\n",
      "Dodano batch 87: dokumenty 430000 - 435000\n",
      "Dodano batch 88: dokumenty 435000 - 440000\n",
      "Dodano batch 89: dokumenty 440000 - 445000\n",
      "Dodano batch 90: dokumenty 445000 - 450000\n",
      "Dodano batch 91: dokumenty 450000 - 455000\n",
      "Dodano batch 92: dokumenty 455000 - 460000\n",
      "Dodano batch 93: dokumenty 460000 - 465000\n",
      "Dodano batch 94: dokumenty 465000 - 470000\n",
      "Dodano batch 95: dokumenty 470000 - 475000\n",
      "Dodano batch 96: dokumenty 475000 - 480000\n",
      "Dodano batch 97: dokumenty 480000 - 485000\n",
      "Dodano batch 98: dokumenty 485000 - 490000\n",
      "Dodano batch 99: dokumenty 490000 - 495000\n",
      "Dodano batch 100: dokumenty 495000 - 500000\n",
      "Dodano batch 101: dokumenty 500000 - 505000\n",
      "Dodano batch 102: dokumenty 505000 - 510000\n",
      "Dodano batch 103: dokumenty 510000 - 515000\n",
      "Dodano batch 104: dokumenty 515000 - 520000\n",
      "Dodano batch 105: dokumenty 520000 - 525000\n",
      "Dodano batch 106: dokumenty 525000 - 530000\n",
      "Dodano batch 107: dokumenty 530000 - 535000\n",
      "Dodano batch 108: dokumenty 535000 - 540000\n",
      "Dodano batch 109: dokumenty 540000 - 545000\n",
      "Dodano batch 110: dokumenty 545000 - 550000\n",
      "Dodano batch 111: dokumenty 550000 - 555000\n",
      "Dodano batch 112: dokumenty 555000 - 560000\n",
      "Dodano batch 113: dokumenty 560000 - 565000\n",
      "Dodano batch 114: dokumenty 565000 - 570000\n",
      "Dodano batch 115: dokumenty 570000 - 575000\n",
      "Dodano batch 116: dokumenty 575000 - 580000\n",
      "Dodano batch 117: dokumenty 580000 - 585000\n",
      "Dodano batch 118: dokumenty 585000 - 590000\n",
      "Dodano batch 119: dokumenty 590000 - 595000\n",
      "Dodano batch 120: dokumenty 595000 - 600000\n",
      "Dodano batch 121: dokumenty 600000 - 605000\n",
      "Dodano batch 122: dokumenty 605000 - 610000\n",
      "Dodano batch 123: dokumenty 610000 - 615000\n",
      "Dodano batch 124: dokumenty 615000 - 620000\n",
      "Dodano batch 125: dokumenty 620000 - 625000\n",
      "Dodano batch 126: dokumenty 625000 - 630000\n",
      "Dodano batch 127: dokumenty 630000 - 635000\n",
      "Dodano batch 128: dokumenty 635000 - 640000\n",
      "Dodano batch 129: dokumenty 640000 - 645000\n",
      "Dodano batch 130: dokumenty 645000 - 650000\n",
      "Dodano batch 131: dokumenty 650000 - 655000\n",
      "Dodano batch 132: dokumenty 655000 - 660000\n",
      "Dodano batch 133: dokumenty 660000 - 665000\n",
      "Dodano batch 134: dokumenty 665000 - 670000\n",
      "Dodano batch 135: dokumenty 670000 - 675000\n",
      "Dodano batch 136: dokumenty 675000 - 680000\n",
      "Dodano batch 137: dokumenty 680000 - 685000\n",
      "Dodano batch 138: dokumenty 685000 - 690000\n",
      "Dodano batch 139: dokumenty 690000 - 695000\n",
      "Dodano batch 140: dokumenty 695000 - 700000\n",
      "Dodano batch 141: dokumenty 700000 - 705000\n",
      "Dodano batch 142: dokumenty 705000 - 710000\n",
      "Dodano batch 143: dokumenty 710000 - 715000\n",
      "Dodano batch 144: dokumenty 715000 - 720000\n",
      "Dodano batch 145: dokumenty 720000 - 725000\n",
      "Dodano batch 146: dokumenty 725000 - 730000\n",
      "Dodano batch 147: dokumenty 730000 - 735000\n",
      "Dodano batch 148: dokumenty 735000 - 740000\n",
      "Dodano batch 149: dokumenty 740000 - 745000\n",
      "Dodano batch 150: dokumenty 745000 - 750000\n",
      "Dodano batch 151: dokumenty 750000 - 755000\n",
      "Dodano batch 152: dokumenty 755000 - 760000\n",
      "Dodano batch 153: dokumenty 760000 - 765000\n",
      "Dodano batch 154: dokumenty 765000 - 769764\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5000  # Zostaw margines poniżej limitu 5461\n",
    "\n",
    "# Podziel dokumenty na partie i dodaj je\n",
    "for i in range(0, len(langchain_documents), BATCH_SIZE):\n",
    "    batch = langchain_documents[i:i + BATCH_SIZE]\n",
    "    vector_store.add_documents(batch)\n",
    "    print(f\"Dodano batch {i//BATCH_SIZE + 1}: dokumenty {i} - {min(i + BATCH_SIZE, len(langchain_documents))}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba8f5d5-aff3-4c5f-ad0d-18b907d8ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769764\n"
     ]
    }
   ],
   "source": [
    "print(vector_store._collection.count())  # Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a499aa-5b3a-4878-b440-2e42dfbe9f13",
   "metadata": {},
   "source": [
    "# **Retrieval System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b3e281e-53c8-4895-b333-40d87bbeff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Between each month comes April?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32c8103d-efe3-4b9e-bd0a-6187e891081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.570056] April comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\n",
      "* [SIM=0.588876] April is the fourth month of the year, and comes between March and May. It is one of four months to have 30 days.\n",
      "* [SIM=0.675394] April begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "* [SIM=0.679126] April is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\n",
      "* [SIM=0.698874] April always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b852c-a733-41f9-97a3-dee889f6ab0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50d98c-587f-49e2-8452-0e4a678d7f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad883397-0042-40bf-aac7-aa21a7aa21c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9d010-005f-4ac0-b99b-d6247c9f1276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9333057-4a18-4eaf-8ef6-5cbfe0ba348b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0cbada-e9bb-4c0d-8cd9-4fe49ba00af9",
   "metadata": {},
   "source": [
    "# **Generation (LLM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4ab17c-4f21-4979-9e2f-c46e112fd29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "# 1. Ładujemy tokenizer i model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# 2. Tworzymy pipeline z odpowiednimi parametrami\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # GPU\n",
    "    do_sample=True,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "# 3. Przekazujemy pipeline do LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a78bda06-aa34-4de5-81dd-1036275d63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeśli używasz notebooka lub Colaba\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a7aa51-6b27-41d6-b11b-3dfb1f3e2ddb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m retriever = \u001b[43mvector_store\u001b[49m.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m})\n\u001b[32m      6\u001b[39m my_prompt = PromptTemplate(\n\u001b[32m      7\u001b[39m     input_variables=[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     template=\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33mAnswer:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m rag_chain = RetrievalQA.from_chain_type(\n\u001b[32m     18\u001b[39m     llm=llm,\n\u001b[32m     19\u001b[39m     retriever=retriever,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Use the following context to answer the question below as clearly and concisely as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b2d3a6f-ba82-4f68-a93d-0e56f9401be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "🔍 Odpowiedź:\n",
      " climate change is now a big problem\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "📄 Źródła:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Climate change ...\n",
      "- Climate change means the climate of Earth changing. Climate change is now a big problem. Climate change this century and last century is sometimes called global warming, because the surface of the Earth is getting hotter, because of humans. But thousands and millions of years ago sometimes it was very cold, like ice ages and snowball Earth. ...\n",
      "- People in government and the Intergovernmental Panel on Climate Change (IPCC) are talking about global warming. But governments, companies, and other people do not agree on what to do about it. Some things that could reduce warming are to burn less fossil fuels, grow more trees, eat less meat, and put some carbon dioxide back in the ground. Shading the Earth from some sunlight (this is called geoengineering) could also reduce warming but we don't understand how it might change weather in other ways. Also people could adapt to any temperature changes. The Kyoto Protocol and Paris Agreement try to reduce pollution from the burning of fossil fuels. Most governments have agreed to them but some people in government think nothing should change. The gas produced by cows digestion also causes global warming, because it contains a greenhouse gas called methane. ...\n",
      "- People can also change how they live because of any changes that global warming will bring. For example, they can go to places where the weather is better, or build walls around cities to keep flood water out. Like the preventive measures, these things cost money, and rich people and rich countries will be able to change more easily than the poor.  ...\n",
      "- Climate change has happened constantly over the history of the Earth, including the coming and going of ice ages. But modern climate change is different because people are putting carbon dioxide into the atmosphere very quickly. ...\n"
     ]
    }
   ],
   "source": [
    "# Przykładowe pytanie\n",
    "query = \"What can u say about climate change?\"\n",
    "result = rag_chain(query)\n",
    "\n",
    "# Wynik\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"🔍 Odpowiedź:\\n\", result[\"result\"])\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n📄 Źródła:\")\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content, \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed75be-d4e2-408f-8152-1efa09d9c504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb75f42-5d3d-424a-b346-906d4708e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa65114-d460-4096-a4a5-49ce08857190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f287f-7db2-4bde-9113-4a8270345eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79952e-c70f-4657-a4db-19e539ff88ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7cdeb-2f65-4e27-8417-2969b6df53c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d9bb9-87f4-4152-940b-3f7277c6eb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c12c0-c4e8-4418-aa0f-b2c0caa6df94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c43d2f-f5a5-4dd5-9d92-ea146e75f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4.43483-a187df25c\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.hip)  # sprawdza, czy ROCm jest dostępny\n",
    "print(torch.cuda.is_available())  # może też zadziałać z ROCm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "696f9a43-e525-4deb-bdb0-806421a2261b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "890f6a50-5f69-4cd7-83b1-d2e5fa947f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: AMD Radeon RX 6750 XT\n",
      "PyTorch version: 2.9.0.dev20250715+rocm6.4\n"
     ]
    }
   ],
   "source": [
    "# Sprawdź szczegóły GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0544479-1bfe-46ba-b952-5e3f28ff2456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08ede7-bfe7-4195-8d3a-4c81bb8e1c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
