{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3ba8b6-113b-4905-8975-644ed1b052df",
   "metadata": {},
   "source": [
    "## Before building the RAG system, we need to check which device we're using ‚Äî GPU or CPU. This helps us make sure that the model runs efficiently and uses available hardware. If a GPU is available, we'll use it for faster processing; otherwise, we fall back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29dc7c1-cf71-43e7-a848-7df6bce44063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: AMD Radeon RX 6750 XT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    pipeline_device = 0\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    pipeline_device = -1\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccf73b-e6ee-4579-8ea9-a1b5c4732d57",
   "metadata": {},
   "source": [
    "# **Load data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f727201-0488-48f4-ba93-6085976b80f5",
   "metadata": {},
   "source": [
    "**First, we need to load the dataset. I chose a small dataset from Wikipedia - specifically the \"rahular/simple-wikipedia\" dataset from HuggingFace Hub.**\n",
    "\n",
    "This dataset has English articles written in a simpler way, which makes it easier to work with. It‚Äôs a good choice for testing RAG because it‚Äôs small and doesn‚Äôt take much time or computing power to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034356b-5b3c-4f58-8f50-26339b0fa478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rahular/simple-wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc85e0-f3f9-4fe3-bd29-05d819cac5d6",
   "metadata": {},
   "source": [
    "### **Key Information About Our Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39b2f2b-8578-4de8-a078-c7005b2ef423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Available splits: ['train']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e3fae8-20a1-4cd3-ba14-b8459f1cd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 769764\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length: {len(dataset[\"train\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2039b53a-0f4b-44af-ae10-f800c9abdaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset features: {'text': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46000122-29da-4301-b7ce-22b83b66e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record (index 558): {'text': 'Plants are also multicellular eukaryotic organisms, but live by using light, water and basic elements to make their tissues.'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample record (index 558): {dataset['train'][558]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169c9d9b-9289-492a-8a51-c65df3f7efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty records: 0\n"
     ]
    }
   ],
   "source": [
    "empty = [i for i in range(len(dataset[\"train\"])) if not dataset[\"train\"][i][\"text\"].strip()]\n",
    "print(f\"Empty records: {len(empty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd5799db-2d66-4e84-a116-cee9441245a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data as DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April is the fourth month of the year, and com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April always begins on the same day of week as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April's flowers are the Sweet Pea and Daisy. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April comes between March and May, making it t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                              April\n",
       "1  April is the fourth month of the year, and com...\n",
       "2  April always begins on the same day of week as...\n",
       "3  April's flowers are the Sweet Pea and Daisy. I...\n",
       "4  April comes between March and May, making it t..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset[\"train\"][:5])\n",
    "print(\"Sample data as DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78c5a0-4764-42bf-82ed-3862c50c8a79",
   "metadata": {},
   "source": [
    "# **Document Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f444701-c18d-4728-af7c-72ada3a9fc54",
   "metadata": {},
   "source": [
    "In this step, I turn each article from the dataset into a Document object. Each one stores the text and a bit of extra info like where it came from and how long it is. This format makes it easier to later add the documents to a vector store and use them in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6718f04-76e9-4a82-a2c4-9f560290f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 769764 LangChain documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document as LangChainDocument\n",
    "    \n",
    "\n",
    "langchain_documents = []\n",
    "for i, record in enumerate(dataset[\"train\"]):\n",
    "    text_content = record[\"text\"]\n",
    "        \n",
    "    if not text_content.strip():\n",
    "        continue\n",
    "        \n",
    "    metadata = {\n",
    "        \"source\": \"simple_wikipedia\",\n",
    "        \"original_index\": i,\n",
    "        \"text_length\": len(text_content),\n",
    "    }\n",
    "        \n",
    "    doc = LangChainDocument(\n",
    "        page_content=text_content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "        \n",
    "    langchain_documents.append(doc)\n",
    "    \n",
    "print(f\"Created {len(langchain_documents)} LangChain documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f4ede-d4ad-4594-9c20-aefbb45ec326",
   "metadata": {},
   "source": [
    "# **Text Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230d506-0080-47f4-ac40-a48595c00cee",
   "metadata": {},
   "source": [
    "Text splitting is used to break up longer documents into smaller chunks. This helps the retriever find more relevant parts of the text later on. Without splitting, long texts might be skipped or give worse results because the important info gets buried inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1413594-99d7-41d6-a391-08a469793b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-empty documents: 769764\n",
      "\n",
      "DOCUMENT LENGTH STATISTICS (in characters):\n",
      "--------------------------------------------------\n",
      "Minimum length: 1 characters\n",
      "Maximum length: 10,570 characters\n",
      "Average length: 183 characters\n",
      "Median length: 127 characters\n",
      "Standard deviation: 198 characters\n",
      "\n",
      "Percentiles:\n",
      "  10th percentile: 12 characters\n",
      "  25th percentile: 24 characters\n",
      "  50th percentile: 127 characters\n",
      "  75th percentile: 271 characters\n",
      "  90th percentile: 432 characters\n",
      "  95th percentile: 558 characters\n",
      "  99th percentile: 872 characters\n"
     ]
    }
   ],
   "source": [
    "text_lengths = []\n",
    "for i, record in enumerate(dataset[\"train\"]):\n",
    "    text = record[\"text\"]\n",
    "    if text.strip():  \n",
    "        text_lengths.append(len(text))\n",
    "\n",
    "print(f\"Total non-empty documents: {len(text_lengths)}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nDOCUMENT LENGTH STATISTICS (in characters):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Minimum length: {min(text_lengths):,} characters\")\n",
    "print(f\"Maximum length: {max(text_lengths):,} characters\")\n",
    "print(f\"Average length: {np.mean(text_lengths):,.0f} characters\")\n",
    "print(f\"Median length: {np.median(text_lengths):,.0f} characters\")\n",
    "print(f\"Standard deviation: {np.std(text_lengths):,.0f} characters\")\n",
    "\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(text_lengths, p)\n",
    "    print(f\"  {p}th percentile: {value:,.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f1a16-e948-4b90-8297-b4d1a30e290f",
   "metadata": {},
   "source": [
    "### Why Text Splitting is Not Necessary for Our Dataset\n",
    "\n",
    "After analyzing our Simple Wikipedia dataset, we found that text splitting (chunking) is not necessary for this particular dataset. Our dataset contains 769,764 non-empty documents with an average length of only 183 characters (46 tokens) and a median of 127 characters (32 tokens). Even the 95th percentile is just 558 characters (140 tokens), which is well below the typical embedding model limits of 512-8192 tokens.\n",
    "\n",
    "\n",
    "Since most embedding models can easily handle documents of this size, splitting our already short Wikipedia articles would actually be counterproductive. Text splitting is typically needed when documents are very long (>1000 characters) or exceed model token limits, but our dataset doesn't meet these criteria. Keeping the documents intact preserves the complete context of each Wikipedia article, results in better embeddings, and simplifies our RAG pipeline. We can proceed directly to embedding generation without any chunking steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d71f7-c09d-4a2d-be58-06c985bbb5d8",
   "metadata": {},
   "source": [
    "# **Embedding Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21561bb9-d726-4b5f-b8a3-804bd4f3f663",
   "metadata": {},
   "source": [
    "In this step, I convert each text chunk into a vector of numbers, called an embedding. These vectors capture the meaning of the text and make it possible to compare them later when searching for relevant content. It‚Äôs a key part of how the system knows which documents are similar to a user‚Äôs question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb3d9d6-5711-4ec9-b63f-8de6524c75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs={'device': device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5950d04a-e320-4979-9bb4-f1cea74b3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in langchain_documents]\n",
    "embeddings = embedding_function.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11144439-22df-4e94-aa81-0fe0c7fe7c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 769764\n",
      "Embedding dimension: 384\n",
      "Shape equivalent: (769764, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"Shape equivalent: ({len(embeddings)}, {len(embeddings[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384eea30-2d20-4d57-b220-0ed6c6407b6b",
   "metadata": {},
   "source": [
    "### **Semantic Similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d6b0c-73fe-473f-b0a8-3daeb586039d",
   "metadata": {},
   "source": [
    "To better understand how embeddings capture meaning, I ran a simple test with three example sentences. Two of them were about climate change, and one was about chocolate cake. Using cosine similarity, the two climate-related texts showed a high similarity score (~0.69), while comparisons with the dessert sentence returned values close to zero. This confirms that text embeddings can effectively group semantically related content and separate unrelated topics ‚Äî a key feature that makes them useful for document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798ee883-15ad-41c4-bb22-8db5e2dffda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Climate change leads to rising global temperatures and extreme weather.\",\n",
    "    \"Greenhouse gases are the main cause of global warming.\",\n",
    "    \"Chocolate cake is a popular dessert made with cocoa powder and sugar.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d194279a-39c0-49a5-9fee-e3c4878df9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = embedding_function.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f93307-51ba-4640-abb3-8d5669092c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9b71ff-6301-40c0-a4e7-063650a5a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between:\n",
      " - \"Climate change leads to rising global temperatures and extreme weather.\"\n",
      " - \"Greenhouse gases are the main cause of global warming.\"\n",
      " = 0.6898\n",
      "\n",
      "Similarity between:\n",
      " - \"Climate change leads to rising global temperatures and extreme weather.\"\n",
      " - \"Chocolate cake is a popular dessert made with cocoa powder and sugar.\"\n",
      " = 0.0066\n",
      "\n",
      "Similarity between:\n",
      " - \"Greenhouse gases are the main cause of global warming.\"\n",
      " - \"Chocolate cake is a popular dessert made with cocoa powder and sugar.\"\n",
      " = -0.0113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(i + 1, len(sentences)):\n",
    "        print(f\"Similarity between:\\n - \\\"{sentences[i]}\\\"\\n - \\\"{sentences[j]}\\\"\\n = {similarity[i][j]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce21bd-7ca4-465d-95f7-c62721e525c2",
   "metadata": {},
   "source": [
    "# **Vector Database Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189da7b2-9163-4d44-97b2-12d0a5e97661",
   "metadata": {},
   "source": [
    "Here, I set up a vector database using Chroma. It stores all the document embeddings so they can be searched later. I load the documents in batches and save everything to disk, which makes the data persistent and ready for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa2ce52-97e1-435f-a1da-59b9a6e2b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "persist_dir = \"./chroma_wikipedia_db\"\n",
    "\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"simple_wikipedia_collection\",\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=persist_dir, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4dc5cf-7e91-4ab6-a14f-5247fa23b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5000  \n",
    "\n",
    "for i in range(0, len(langchain_documents), BATCH_SIZE):\n",
    "    batch = langchain_documents[i:i + BATCH_SIZE]\n",
    "    vector_store.add_documents(batch)\n",
    "    #print(f\"Added batch {i//BATCH_SIZE + 1}: documents {i} - {min(i + BATCH_SIZE, len(langchain_documents))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba8f5d5-aff3-4c5f-ad0d-18b907d8ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents stored in the vector database: 769764\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of documents stored in the vector database: {vector_store._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a499aa-5b3a-4878-b440-2e42dfbe9f13",
   "metadata": {},
   "source": [
    "# **Retrieval System**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ab385-7f3e-4fc2-b53d-d5fb9e74293e",
   "metadata": {},
   "source": [
    "In this part, I test the retrieval by asking a sample question. The system searches the vector database and returns the most similar documents based on the query. Each result comes with a similarity score that shows how closely it matches the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b3e281e-53c8-4895-b333-40d87bbeff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Between each month comes April?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32c8103d-efe3-4b9e-bd0a-6187e891081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.570056] April comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\n",
      "* [SIM=0.588876] April is the fourth month of the year, and comes between March and May. It is one of four months to have 30 days.\n",
      "* [SIM=0.675394] April begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "* [SIM=0.679126] April is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\n",
      "* [SIM=0.698874] April always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cbada-e9bb-4c0d-8cd9-4fe49ba00af9",
   "metadata": {},
   "source": [
    "# **Generation (LLM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c30099-e36f-4648-a851-78acc1e2575b",
   "metadata": {},
   "source": [
    "In this stage, I load a language model (flan-t5-large) and connect it to the retrieval system using LangChain. The model is responsible for generating final answers based on the documents retrieved earlier. I also define a custom prompt that tells the model how to respond using the given context. Then I build a RAG chain, which combines the retriever and the generator into one workflow. When I ask a question, the system finds relevant documents, passes them to the model, and returns a generated answer along with the sources it used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a4ab17c-4f21-4979-9e2f-c46e112fd29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=pipeline_device,  \n",
    "    do_sample=True,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12a7aa51-6b27-41d6-b11b-3dfb1f3e2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template = \"\"\"\n",
    "You are a helpful assistant. Using only the information below, write a detailed and informative answer to the question.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b2d3a6f-ba82-4f68-a93d-0e56f9401be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11744/1502515873.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "üîç Result:\n",
      " The central theme of hydrology is how the water circulates. This is called the water cycle. The most vivid illustration of it is the water evaporation from the ocean with the formation of clouds. These clouds drift over the land and produce rain.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Source:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Water cycle ...\n",
      "- The water cycle (or hydrological cycle) is the cycle that water goes through on Earth. ...\n",
      "- This is the process that water starts and ends in the water cycle. ...\n",
      "- Human activities that change the water cycle include: ...\n",
      "- The central theme of hydrology is how the water circulates. This is called the water cycle. The most vivid illustration of it is the water evaporation from the ocean with the formation of clouds. These clouds drift over the land and produce rain. ...\n",
      "- Evaporation is a very essential part of the water cycle. ...\n",
      "- The air animals and plants use to live is only the first level of the air around the Earth (the troposphere). The day to day changes in this level of air are named weather; the changes between places far away from each other and from year to year are named the climate. Rain and storms are both in this level. Both come about because this part of the air gets colder as it goes up. Cold air becomes thicker and falls, and warm air becomes thinner and goes up. The turning Earth moves the air as well and air moves north and south because the middle of the Earth generally gets more power from the Sun and is warmer than the north and south points. At the same time, air over water (specially very warm water) gets water in it but, because cold air is not able to take in as much water, it starts to make clouds and rain as it gets colder. The way water moves around in a circle like this is called the water cycle. ...\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the water cycle work in nature?\"\n",
    "result = rag_chain(query)\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"üîç Result:\\n\", result[\"result\"])\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nüìÑ Source:\")\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content, \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
